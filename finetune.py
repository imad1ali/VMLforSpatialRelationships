# -*- coding: utf-8 -*-
"""Untitled10.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-swK9r2iR0-lrx0aWlkioq6P6NNx2WxQ

inspired by https://huggingface.co/learn/cookbook/en/fine_tuning_smol_vlm_sft_trl
"""

!sudo update-alternatives --config python3
!python --version

!curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
!python get-pip.py
!pip install git+https://github.com/huggingface/accelerate.git
!pip install git+https://github.com/huggingface/transformers.git
!pip install bitsandbytes
!pip install trl
!pip install peft

access token - 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

from huggingface_hub import notebook_login
notebook_login()

system_message = """
You are a Vision-Language Model trained to describe pairwise spatial relationships between objects in an image.

### Your task:
- Analyze the provided image.
- Identify all visible pairwise object relationships.

### Strict output format:
- Write each relation exactly as: [obj1] [relationship] [obj2]
- Do NOT include brackets in the output (they denote placeholders).
- Use short, lowercase object names.
- Separate relations with a comma and a single space.
- Use only these relationships (exact strings): "to the left", "to the right", "on top".
- Output exactly one line with no extra words or punctuation.
- Do not describe the objects just provide the name

### Examples:
mug on top book, mug to the left laptop, chair to the right table
"""
system_message = "hello"

from PIL import Image

def format_data(sample):
    # You will need to adapt this function to the structure of your training data.
    # Replace the placeholder comments with code to access the image, query, and label
    # from your 'sample' object.

    # Example: If your sample is a dictionary with keys 'image_data', 'question', and 'answer':
    # image = sample['image_data']
    # query = sample['question']
    # label = sample['answer']

    # Replace these placeholders with your data access logic
    image_path = sample["image"] # Access the image path from your sample
    image = Image.open(image_path) # Open the image file
    query = sample['question'] # Access the query from your sample (changed from 'query')
    label = sample["answer"] # Access the label (answer) from your sample (changed from 'label', and removed [0] indexing)

    return [
        {
            "role": "system",
            "content": [
                {"type": "text", "text": system_message}
            ],
        },
        {
            "role": "user",
            "content": [
                {"type": "image", "image": image}
            ],
        },
        {
            "role": "assistant",
            "content": [
                {"type": "text", "text": label}
            ],
        },
    ]

import pandas as pd

# Load the metadata from the JSON file
metadata_df = pd.read_json('metadata_all.json')

# Apply the format_data function to create the train_dataset
# Assuming your JSON structure has a list of records where each record can be passed to format_data
train_dataset = [format_data(sample) for sample in metadata_df.to_dict('records')]

# You would similarly load and format your evaluation and test datasets here
# eval_dataset = [format_data(sample) for sample in eval_metadata_df.to_dict('records')]
# test_dataset = [format_data(sample) for sample in test_metadata_df.to_dict('records')]

# For now, let's create empty lists for eval and test datasets if they are not loaded
# If you have separate files for eval and test, uncomment and modify the lines above
if 'eval_dataset' not in locals():
    eval_dataset = []
if 'test_dataset' not in locals():
    test_dataset = []

print(f"Loaded {len(train_dataset)} samples for training.")
print(f"Loaded {len(eval_dataset)} samples for evaluation.")
print(f"Loaded {len(test_dataset)} samples for testing.")

import torch
from transformers import Idefics3ForConditionalGeneration, AutoProcessor

model_id = "HuggingFaceTB/SmolVLM-256M-Instruct"

model = Idefics3ForConditionalGeneration.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    #_attn_implementation="flash_attention_2",
)

processor = AutoProcessor.from_pretrained(model_id)

train_dataset[0]

def generate_text_from_sample(model, processor, sample, max_new_tokens=1024, device="cuda"):
    # Prepare the text input by applying the chat template
    text_input = processor.apply_chat_template(sample[:2], add_generation_prompt=False)
    print(text_input)

    image_inputs = []
    image = sample[1]['content'][0]['image']
    if image.mode != 'RGB':
        image = image.convert('RGB')
    image_inputs.append([image])

    # Prepare the inputs for the model
    model_inputs = processor(
        #text=[text_input],
        text=text_input,
        images=image_inputs,
        return_tensors="pt",
    ).to(device)  # Move inputs to the specified device

    # Generate text with the model
    generated_ids = model.generate(**model_inputs, max_new_tokens=max_new_tokens)

    # Trim the generated ids to remove the input ids
    trimmed_generated_ids = [
        out_ids[len(in_ids):] for in_ids, out_ids in zip(model_inputs.input_ids, generated_ids)
    ]

    # Decode the output text
    output_text = processor.batch_decode(
        trimmed_generated_ids,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False
    )

    return output_text[0]  # Return the first decoded output text

output = generate_text_from_sample(model, processor, train_dataset[5], max_new_tokens=128, device="cuda")
print(output)

import gc
import time

def clear_memory():
    # Delete variables if they exist in the current global scope
    if 'inputs' in globals(): del globals()['inputs']
    if 'model' in globals(): del globals()['model']
    if 'processor' in globals(): del globals()['processor']
    if 'trainer' in globals(): del globals()['trainer']
    if 'peft_model' in globals(): del globals()['peft_model']
    if 'bnb_config' in globals(): del globals()['bnb_config']
    time.sleep(2)

    # Garbage collection and clearing CUDA memory
    gc.collect()
    time.sleep(2)
    torch.cuda.empty_cache()
    torch.cuda.synchronize()
    time.sleep(2)
    gc.collect()
    time.sleep(2)

    print(f"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB")
    print(f"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB")

clear_memory()

import torch
from transformers import BitsAndBytesConfig


# BitsAndBytesConfig int-4 config
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# Load model and tokenizer
model = Idefics3ForConditionalGeneration.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    quantization_config=bnb_config,
    #_attn_implementation="flash_attention_2",
)
processor = AutoProcessor.from_pretrained(model_id)

from peft import LoraConfig, get_peft_model

# Configure LoRA
peft_config = LoraConfig(
    r=8,
    lora_alpha=8,
    lora_dropout=0.1,
    target_modules=['down_proj','o_proj','k_proj','q_proj','gate_proj','up_proj','v_proj'],
    use_dora=True,
    init_lora_weights="gaussian"
)

# Apply PEFT model adaptation
peft_model = get_peft_model(model, peft_config)

# Print trainable parameters
peft_model.print_trainable_parameters()

from trl import SFTConfig

# Configure training arguments using SFTConfig
training_args = SFTConfig(
    output_dir="smolvlm-instruct-trl-sft-ChartQA",
    num_train_epochs=1,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    warmup_steps=50,
    learning_rate=1e-4,
    weight_decay=0.01,
    logging_steps=25,
    save_strategy="steps",
    save_steps=25,
    save_total_limit=1,
    optim="adamw_torch_fused",
    bf16=True,
    push_to_hub=True,
    report_to="tensorboard",
    remove_unused_columns=False,
    gradient_checkpointing=True,
    dataset_text_field="",
    dataset_kwargs={"skip_prepare_dataset": True},
)

image_token_id = processor.tokenizer.additional_special_tokens_ids[
            processor.tokenizer.additional_special_tokens.index("<image>")]

def collate_fn(examples):
    texts = [processor.apply_chat_template(example, tokenize=False) for example in examples]

    image_inputs = []
    for example in examples:
      image = example[1]['content'][0]['image']
      if image.mode != 'RGB':
          image = image.convert('RGB')
      image_inputs.append([image])

    batch = processor(text=texts, images=image_inputs, return_tensors="pt", padding=True)
    labels = batch["input_ids"].clone()
    labels[labels == processor.tokenizer.pad_token_id] = -100  # Mask padding tokens in labels
    labels[labels == image_token_id] = -100  # Mask image token IDs in labels

    batch["labels"] = labels

    return batch

from trl import SFTTrainer

trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=collate_fn,
    peft_config=peft_config,
    processing_class=processor.tokenizer,
)

trainer.train()

trainer.save_model(training_args.output_dir)

model = Idefics3ForConditionalGeneration.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    #_attn_implementation="flash_attention_2",
)

processor = AutoProcessor.from_pretrained(model_id)

adapter_path = "smolvlm-instruct-trl-sft-ChartQA"
model.load_adapter(adapter_path)

train_dataset[0][1]['content'][0]['image']

output = generate_text_from_sample(model, processor, train_dataset[2])
output